# ğŸš€ Modern Data Engineering Pipeline Project

<p align="center">
  <img src="Img/Pipeline.png" alt="Dashboard Preview" width="80%" />
</p>

---

## ğŸ“Œ Overview

This project showcases a modern **Data Engineering** pipeline built using:

- ğŸŒ¨ï¸ **Snowflake** â€“ Cloud data warehouse  
- ğŸ› ï¸ **dbt (Data Build Tool)** â€“ Data transformation and modeling  
- ğŸ•¸ï¸ **Apache Airflow** â€“ Workflow orchestration using **CeleryExecutor**  
- ğŸ“Š **Power BI** â€“ Data visualization and dashboards  
- ğŸ³ **Docker** â€“ Environment containerization

### ğŸ“ Input Data

The project uses the following CSV files:

- `orders.csv`  
- `order_items.csv`  
- `products.csv`  
- `customers.csv`

These are uploaded to a Snowflake internal stage (`sales_stage`) and processed into a structured data model via dbt.

---

## ğŸ—‚ï¸ Project Structure

```bash
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt_dag.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â”œâ”€â”€ stg_customers.sql
â”‚       â”‚   â”œâ”€â”€ stg_orders.sql
â”‚       â”‚   â”œâ”€â”€ stg_order_items.sql
â”‚       â”‚   â””â”€â”€ stg_products.sql
â”‚       â””â”€â”€ marts/
â”‚           â”œâ”€â”€ Countries_Quantities.sql
â”‚           â”œâ”€â”€ Customer_Segmentation.sql
â”‚           â”œâ”€â”€ Daily_Order_Revenue.sql
â”‚           â””â”€â”€ Status_Order_Count.sql
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ snowflake_test.yml
â”œâ”€â”€ airflow_cfg/
â”œâ”€â”€ airflow_db/
â”‚   â””â”€â”€ airflow.db




```

âš™ï¸ Tech Stack
ğŸ”§ Tool	ğŸ’¼ Purpose
Snowflake	Cloud Data Warehouse
dbt	Data Modeling & Transformation
Apache Airflow	Workflow Orchestration
Power BI	Business Intelligence & Dashboard
Docker	Containerized Development

â„ï¸ Snowflake Workflow
ğŸ“¤ Upload CSVs to sales_stage (internal stage)

ğŸ§± Create raw tables: customers, orders, order_items, products

ğŸ“¥ Load data with COPY INTO

ğŸ”„ Transform using dbt staging models

ğŸ§® Create final marts models for analytics

ğŸ§± dbt Models
ğŸ”¹ Staging Models (models/staging/)
stg_customers.sql

stg_orders.sql

stg_order_items.sql

stg_products.sql

â¡ï¸ These clean, standardize, and prepare raw Snowflake data.

ğŸ”¸ Mart Models (models/marts/)
Countries_Quantities.sql: ğŸ“¦ Product quantities per country

Customer_Segmentation.sql: ğŸ‘¥ Customer classification

Daily_Order_Revenue.sql: ğŸ“ˆ Daily revenue trends

Status_Order_Count.sql: ğŸ“Š Order status breakdown

âœ… dbt Testing (tests/snowflake_test.yml)
âœ”ï¸ Validates order_status values (Completed, Pending, Cancelled)

âœ”ï¸ Enforces uniqueness of customer_id in stg_customers

â° Airflow Integration
Airflow orchestrates dbt model runs using:

Webserver, Scheduler, Worker & Flower

DAG File: dags/dbt_dag.py

Executor: CeleryExecutor

ğŸ” Start Airflow
bash
Copy
Edit
docker-compose up airflow-init   # Run once
docker-compose up                # Start services
ğŸŒ Access UI: http://localhost:8081

ğŸ“Š Power BI Dashboard
After dbt materializes models in Snowflake:

ğŸ§© Connect Power BI using native Snowflake connector

ğŸ“¥ Import tables/views like:

daily_order_revenue

customer_segmentation

status_order_count

countries_quantities

ğŸ–¼ï¸ Build dashboards with:

ğŸ“ˆ Revenue Trends

ğŸ“Š Order Status Summary

ğŸŒ Country-wise Product Quantities

ğŸ‘¥ Customer Segments

ğŸ³ Docker Setup
ğŸ—ï¸ Build & Run Containers
bash
Copy
Edit
docker-compose build
docker-compose up
ğŸŒ Environment Variables (.env)
env
Copy
Edit
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
ğŸ“ Notes
âœ… Ensure Snowflake credentials are set in profiles.yml

ğŸ“‚ Run dbt commands from /opt/airflow/dbt inside the container

ğŸ” dbt automatically manages lineage and model dependencies

ğŸš€ Future Enhancements
âœ… Integrate Great Expectations for data quality checks

ğŸ“¥ Automate file arrival triggers or API ingestion

ğŸ”„ Add CI/CD with GitHub Actions or Jenkins

